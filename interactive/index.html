<!DOCTYPE html>
<meta charset="utf-8">
<div class="settings">
    <head> 
        <title> Multi-Arm Bandit Problem </title>
    </head>
    <body>
        <h1>Multi-Arm Bandit Problem</h1>
        <p>In layman's terms, the multi-armed bandit problem can be conceptualized as a person picking amongst a finite number of 
            slot machines at a Las Vegas Casino. With partial, prior knowledge of the rewards each slot machine yields, the person 
            must make a concious decision to maximize their reward. </p>
        <p> In machine learning, this problem is very similar: a finite quantity of rewards are distributed amongst various arms, or choices, 
            and the rewards must be sequentially selected in a way that maximizes the total reward, while the rewards generated from each 
            arm are not fully understood but improve as the number of steps (or choice selections) increases.
        </p>
        <p>
            The rewards generated from each arm are sampled from a normal distribution of the rewards for that particular arm, and 
            the means and variances of these distributions remain constant throughout the experiment. The problem in such a setup 
            is known as the exploration-exploitation dilemna. Exploitation occurs when an agent repeatedly select the arm with the 
            highest average reward after a certain number of steps, whereas exploration results in a random selection of the available 
            arms. The dilmena arises since repeatedly exploiting could cause an algorithm to miss out on opportunities to select the true 
            highest reward, while continuously exploring can result in subpar rewards each time. Learning more about the optimal balance of 
            exploration vs exploitation is a subject of ongoing research. 
        </p>
        <p>
            The value ε is a probability on [0,1) that is selected prior to the experiment. During each successive run, a random number from [0,1) is selected. By convention, if the random number 
            is less than ε, the agent explores. Otherwise, it exploits. The following interactive tool demonstrates the epsilon-greedy 
            algorithm. Feel free to adjust the number of selections or the epsilon value. A graph of the average reward (total reward / 
            number of steps) is shown.
        </p>
        <div style="width:100%">
		    <svg width="600" height="300">
                <g id="scores">
                    <text id="yScore" x="0" y="20" fill="royalblue">Your score: 0</text><text id="aScore" x="0" y="40" fill="crimson">Algorithm score: 3</text>
                </g>
                <g id="machines">
                    <g id="machine0">
                        <rect id="machineBody0" stroke="black" x="50" y="150" width="100" height="50" fill="white" rx="5" ry="5" onclick="alert('You have clicked the circle.')"></rect>  
                        <text x="80" y="182" fill="black">Hello</text>                      
                    </g>
                    <g id="machine1">
                        <rect id="machineBody0" stroke="black" x="200" y="150" width="100" height="50" fill="white" rx="5" ry="5" onclick="alert('You have clicked the circle.')"></rect> 
                        <text x="230" y="182" fill="black">Hello</text>                         
                    </g>
                    <g id="machine2">
                        <rect id="machineBody0" stroke="black" x="350" y="150" width="100" height="50" fill="white" rx="5" ry="5" onclick="alert('You have clicked the circle.')"></rect> 
                        <text x="380" y="182" fill="black">Hello</text>                         
                    </g>
            </svg>
        </div>
        <script src="https://d3js.org/d3.v4.min.js"></script>
        <script src="interactive.js"></script>
    </body>
</html>
